s2_step = s2_start^2
s2_step = s2_start^2
tau_step = tau_start
m1_start= 6
m1_start= 6
m2_start = -1
s1_start = 1
s2_start = 2
tau_start = 0.4
m1_step = m1_start
m1_step = m1_start
m2_step = m2_start
s1_step = s1_start^2
s2_step = s2_start^2
tau_step = tau_start
p = p_fun(x, m1_step, s1_step, m2_step, s2_step, tau_step)
p
m1 = sum(x * (1 -p)) / sum((1 -p))
m2 = sum(x * p )/ sum(p)
s1 = sum((1 -p) * (x-m1)^2)/ sum(1-p)
s1 = sum((1 -p) * (x-m1)^2)/ sum(1-p)
s2 = sum(p * (x-m2)^2)/sum(p)
tt = sum(p)/N
p
m1
m2
s1
s2
tt
theta_df = tibble(mu_1 = m1_start, s_1 = s1_start, mu_2 = m2_start, s_2 = s2_start, tau = tau_start)
while (abs(llik(x, m1, m2, s1, s2, tt) - llik(x, m1_step, m2_step, s1_step, s2_step, tau_step)) > 10^-10){
m1_step = m1
m2_step = m2
s1_step = s1
s2_step = s2
tau_step = tt
p = p_fun(x,m1_step, s1_step, m2_step, s2_step, tau_step)
m1 = sum(x * (1 -p)) / sum((1 -p))
m2 = sum(x * p )/ sum(p)
s1 = sum((1 -p) * (x-m1)^2)/ sum(1-p)
s2 = sum(p * (x-m2)^2)/sum(p)
tt = sum(p)/N
theta_df = theta_df %>% add_row(mu_1 = m1, s_1 = sqrt(s1), mu_2 = m2, s_2 = sqrt(s2), tau = tt)
}
theta_df
View(theta_df)
theta2_df = get_theta(X, 6, -1, 1, 2, 0.4)
theta2 = theta2_df[nrow(theta2_df),]
theta2
theta3_df = get_theta(X, 20, -10, 1, 2, 0.4)
theta3_df
theta2_df$iteration = 0:(nrow(theta2_df)-1)
theta2_df = get_theta(X, 6, -1, 1, 2, 0.4)
theta2 = theta2_df[nrow(theta2_df),]
theta2
theta2_df$iteration = 0:(nrow(theta2_df)-1)
m1_plot = ggplot(theta2_df) +
theme_bw() +
aes(iteration, mu_1) +
geom_line(linewidth = 0.7, col ="#00BE6C") +
geom_hline(yintercept = mu1, color = "#FC717F", linewidth = 0.7) +
labs(x = "Iteration", y = expression(hat(mu1)), subtitle = "In red the true mean") +
ggtitle("Estimates of mu1") +
theme(plot.title = element_text(face = "bold"))
m2_plot = ggplot(theta2_df) +
theme_bw() +
aes(iteration, mu_2) +
geom_line(linewidth = 0.7, col ="#00BE6C") +
geom_hline(yintercept = mu2, color = "#FC717F", linewidth = 0.7) +
labs(x = "Iteration", y = expression(hat(mu2)), subtitle = "In red the true mean") +
ggtitle("Estimates of mu2") +
theme(plot.title = element_text(face = "bold"))
s1_plot = ggplot(theta2_df) +
theme_bw() +
aes(iteration, s_1) +
geom_line(linewidth = 0.7, col ="#00B4EF") +
geom_hline(yintercept = sigma1, color = "#FC717F", linewidth = 0.7) +
labs(x = "Iteration", y = expression(hat(s1)), subtitle = "In red the true standard deviation") +
ggtitle("Estimates of sigma1") +
theme(plot.title = element_text(face = "bold"))
s2_plot = ggplot(theta2_df) +
theme_bw() +
aes(iteration, s_2) +
geom_line(linewidth = 0.7, col ="#00B4EF") +
geom_hline(yintercept = sigma2, color = "#FC717F", linewidth = 0.7) +
labs(x = "Iteration", y = expression(hat(s2)), subtitle = "In red the true standard deviation") +
ggtitle("Estimates of sigma2") +
theme(plot.title = element_text(face = "bold"))
grid.arrange(m1_plot, s1_plot, m2_plot, s2_plot, nrow = 2)
renv::init()
renv::init()
install.packages("renv")
renv::init()
y
renv::init()
renv::init(force = TRUE)
?renv::dependencies
install.packages("slam")
library(slam)
install.packages("C:/gurobi1003/win64/R/gurobi_10.0-3.zip", repos = NULL)
library(bestsubset)
library(MLGdata)
Beetles
y<-Beetles$uccisi/Beetles$num
y
plot(Beetles$logdose, y)
#trasformo i dati con le funzioni legame: legame canonica logit
logit=log(y/(1-y))
y
#quando houna proporzione che e 1 il logaritmo va ad infinito e perdo un dato
plot(Beetles$logdose, logit) #perdo un dato
logitc <- with(Beetles, log((y+0.5/num)/(1-y+0.5/num)))
logitc
plot(Beetles$logdose, logitc)
#relazione crescente la logit e probabilita
#relazione crescente logdose e probabilita di morire
#potrebbero esserci relazioni migliori della lineare
#per capire come andamento lineare si discosti da quello osswervato aggiungo
#una linea di regressione
abline(lm(logitc~Beetles$logdose))
summary(lm(logitc~Beetles$logdose))$r.squared
#la relazione lineare sembra appropriata
summary(lm(logitc~Beetles$logdose))$coefficients
str(summary(lm(logitc~Beetles$logdose))$coefficients)
#vedi pag. 123
#come ottenere la funzione probit
#faccio una correzione arbitraria: aggiungo 0.1
yc <- y+0.1/Beetles$num
yc[y>0.5] <-y[y>0.5]-0.1/Beetles$num[y>0.5]
x11()
plot(Beetles$logdose, logitc, col="red")
# funzione mnotona crescente: al crescere della logdose aumenta la probabilita' di morire
abline(lm(qnorm(yc)~Beetles$logdose))
abline(lm(logitc~Beetles$logdose), col="red")
points(Beetles$logdose, qnorm(yc))
summary(lm(qnorm(yc)~Beetles$logdose))$r.squared
#posso ottenere intervalli di confidenza e di previsione
######  inferenza  ######
# 1. logit
Beetles.glm <- glm(y ~ logdose, family = binomial,
weights = num, data = Beetles)
summary(glm(cbind(uccisi, num-uccisi)~logdose, family=binomial, data=Beetles))
summary(Beetles.glm)
# i due summary sono uguali
#es pag 147
vcov(Beetles.glm)
#inversione di Xt*W*X    matrice=W
#intervallo conf livello 0.95
#eta_j = beta1+ b2*logdose_j
predeta<-predict(Beetles.glm, newdata = data.frame(logdose=1.8), se.fit = TRUE)
ICeta <- predeta$fit+ c(-1,1)*qnorm(0.975)*predeta$se.fit
ICeta #intervallo di confidenza per eta, non per la probabilita di morte degli scarafaggi
plogis(ICeta)
#cio che ottengo e' all' interno dello spazio parametrico delle probabilita
#posso usare il comando predict in cui dico il tipo di previsione che voglio
predpi<- predict(Beetles.glm, newdata = data.frame(logdose=1.8), se.fit = TRUE, type="response")
#mi restituisce una stima puntuale
ICpi1<-predpi$fit+ c(-1,1)*qnorm(0.975)*predpi$se.fit
ICpi<-plogis(ICeta)
#i due intervalli sono diversi
#altro modo:
Beetles.glm$family$linkinv(ICeta)
Beetles.glm2 <- glm(y ~ logdose, weights = num, family=binomial(link=probit), data=Beetles)
summary(Beetles.glm2)
summary(Beetles.glm2)$aic
summary(Beetles.glm)$aic
# modello probit: adattamwento migliore anche se sono quasi equivalenti
#grafici
#vedere come si adattano le stime delle probabilita al variare della dose
pihat <- fitted(Beetles.glm)
plot(Beetles$logdose, y)
lines(Beetles$logdose, pihat, lty=2)
pihat2 <- fitted(Beetles.glm2)
lines(Beetles$logdose, pihat2, lty=3, col="red")
Beetles.glm3 <- glm(y~logdose, weights =num, family=binomial(link=cloglog), data=Beetles)
summary(Beetles.glm3)
pihat3 <- fitted(Beetles.glm3)
#in termini di AIC il modello con log log complementare e' il migliore
x11()
plot(Beetles$logdose, y)
lines(Beetles$logdose, pihat, lty=2)
lines(Beetles$logdose, pihat2, lty=3, col="red")
lines(Beetles$logdose, pihat3, lty=4, col="blue")
legend("bottomright", c("logistica", "probit", "log-log comp"), lty=c(2,3,4), col=c(1, "red", "blue"))
#la probabilita stimata tende a seguire sempre di piu i dati
#adattamento che tende ad essere generalmente migliore
c(summary(Beetles.glm)$aic,
summary(Beetles.glm2)$aic,
summary(Beetles.glm3)$aic)
#altro strumento valutazione bonta di adattamento
#tabella di errata classificazione
#fisso una soglia delle probabilita per dire come classifico l'elemento
#varie strategie per scegliere la soglia: 0,5 , proporzione di uno nel campine, alzo la soglia a 0.99
#auc e curva roc permette di visualizzare come al variare della soglia variano specificita e sensibilita
#un modello con area massima sotto la curva e il modello migliore
library(pROC)
str(Beetles10)
#aggregare:
#pacchetto dplyr::  i due : permette di accedere a funzioni del pacchetto senza fare l'upload
# calcolo curva roc
Beetles10.glm3 <- glm(ucciso ~ log.dose10,
family=binomial(link=cloglog),
data=Beetles10)
#weigthg=1 sempre
summary(Beetles10.glm3)
#dati non aggregati
#non ottengo lo stesso AIC ma il modello sottostante e' lo stesso
#l'effetto che voglio misurare rimane lo stesso, stesse stime
#per ottenere curva ROC:
pihat310 <- fitted(Beetles10.glm3)
plot(roc(Beetles10$ucciso, pihat310), print.auc=TRUE)
#esercizio 3.5
#uso il metodo delta, non e' l'unico metodo
coef <- Beetles.glm$coefficients
X <- model.matrix((Beetles.glm))
X
lasso = function(x, y, alpha=1, nrelax=1, nlambda=50,
lambda.min.ratio=ifelse(nrow(x)<ncol(x),0.01,0.0001),
lambda=NULL, intercept=TRUE, standardize=TRUE) {
# Check for glmnet package
if (!require("glmnet",quietly=TRUE)) {
stop("Package glmnet not installed (required here)!")
}
# Set up data
x = as.matrix(x)
y = as.numeric(y)
n = nrow(x)
p = ncol(x)
# Set dfmax manually
dfmax = p
if (nrelax > 1 && n < (p-intercept)) dfmax = n-intercept
# Reset nlambda if a specific lambda sequence is passed
if (!is.null(lambda)) nlambda = length(lambda)
# weights (all equal to 1 if not adaptive lasso)
w = rep(1, dfmax)
# adaptive lasso
if (penalty == T){
y1 = y - mean(y)
x1 = scale(x)
fit1 = glm(y1 ~ x1)
w = 1/abs(coef(fit1)[-1])
}
# Run glmnet
obj = glmnet(x, y, alpha=alpha, nlambda=nlambda, dfmax=dfmax,
lambda.min.ratio=lambda.min.ratio, lambda=lambda,
intercept=intercept, standardize=standardize)
# Append a few things to the returned object
obj$nrelax = nrelax
obj$nlambda = nlambda
obj$intercept = intercept
obj$x = x; obj$y = y
class(obj) = "lasso"
return(obj)
}
lasso1 = function(x, y, alpha=1, nrelax=1, nlambda=50,
lambda.min.ratio=ifelse(nrow(x)<ncol(x),0.01,0.0001),
lambda=NULL, intercept=TRUE, standardize=TRUE) {
# Check for glmnet package
if (!require("glmnet",quietly=TRUE)) {
stop("Package glmnet not installed (required here)!")
}
# Set up data
x = as.matrix(x)
y = as.numeric(y)
n = nrow(x)
p = ncol(x)
# Set dfmax manually
dfmax = p
if (nrelax > 1 && n < (p-intercept)) dfmax = n-intercept
# Reset nlambda if a specific lambda sequence is passed
if (!is.null(lambda)) nlambda = length(lambda)
# weights (all equal to 1 if not adaptive lasso)
w = rep(1, dfmax)
# adaptive lasso
if (penalty == T){
y1 = y - mean(y)
x1 = scale(x)
fit1 = glm(y1 ~ x1)
w = 1/abs(coef(fit1)[-1])
}
# Run glmnet
obj = glmnet(x, y, alpha=alpha, nlambda=nlambda, dfmax=dfmax,
lambda.min.ratio=lambda.min.ratio, lambda=lambda,
intercept=intercept, standardize=standardize)
# Append a few things to the returned object
obj$nrelax = nrelax
obj$nlambda = nlambda
obj$intercept = intercept
obj$x = x; obj$y = y
class(obj) = "lasso"
return(obj)
}
library(bestsubset)
library(gridExtra)
setwd("C:/Users/Maria Vittoria/Desktop/EPFL/SCV/main-project-mvk")
source("newLasso.R")
#
n = 20
#
n = 10
p = 20
reg.funs[["Best subset"]] = function(x,y) bs(x,y,intercept=FALSE)
reg.funs = list()
reg.funs[["Best subset"]] = function(x,y) bs(x,y,intercept=FALSE)
sim_big = sim.master(n,p,nval,reg.funs=reg.funs,nrep=1,seed=0,
beta.type=2,s=5,snr=1,verbose=TRUE)
nval = n
sim_big = sim.master(n,p,nval,reg.funs=reg.funs,nrep=1,seed=0,
beta.type=2,s=5,snr=1,verbose=TRUE)
sim_big
reg.funs[["Best subset"]] = function(x,y) bs(x,y,intercept=FALSE)
sim_big = sim.master(n,p,nval,reg.funs=reg.funs,nrep=10,seed=0,
beta.type=2,s=5,snr=1,verbose=TRUE)
n
p
plot(sim_big, what ="risk")
#
n = 20
p = 30
sim_big = sim.master(n,p,nval,reg.funs=reg.funs,nrep=10,seed=0,
beta.type=2,s=5,snr=1,verbose=TRUE)
#
n = 20
p = 25
sim_big = sim.master(n,p,nval,reg.funs=reg.funs,nrep=1,seed=0,
beta.type=2,s=5,snr=1,verbose=TRUE)
plot(sim_big, what ="risk")
n
p
load("C:/Users/Maria Vittoria/Desktop/EPFL/SCV/main-project-mvk/workspace.RData")
library(bestsubset)
library(gridExtra)
setwd("C:/Users/Maria Vittoria/Desktop/EPFL/SCV/main-project-mvk")
plot(sim.obj.hisnr, main="SNR = 1") )
plot(sim.obj.hisnr, main="SNR = 1")
plot(sim.obj.losnr, main="SNR = 0.1")
plot(sim.obj.hisnr, method.nums=1:3,  main="SNR = 1") #, legend.pos="topright")
plot(sim.obj.losnr, method.nums=1:3, main="SNR = 0.1") #, legend.pos="topleft")
grid.arrange(fig1.1, fig1.2, ncol = 2 )
library(bestsubset)
?plot
# Regression functions: lasso, forward stepwise, and best subset selection
reg.funs = list()
reg.funs[["Best subset"]] = function(x,y) bs(x,y,intercept=FALSE)
reg.funs[["Stepwise"]] = function(x,y) fs(x,y,intercept=FALSE)
reg.funs[["Lasso"]] = function(x,y) lasso1(x,y,intercept=FALSE)
reg.funs[["Relaxed lasso"]] = function(x,y) lasso1(x,y,intercept=FALSE,
relax=T, gamma = 0.5)
reg.funs[["Adaptive Lasso"]] = function(x,y) lasso1(x,y,intercept=FALSE, penalty =T)
reg.funs[["Elastic Net"]] = function(x,y) lasso1(x,y,intercept=FALSE, alpha = 0.05)
n = 100
p = 20
nval = n
########### FIG 1 #############
set.seed(0)
# Regression functions: lasso, forward stepwise, and best subset selection
reg.funs = list()
reg.funs[["Best subset"]] = function(x,y) bs(x,y,intercept=FALSE)
reg.funs[["Stepwise"]] = function(x,y) fs(x,y,intercept=FALSE)
reg.funs[["Lasso"]] = function(x,y) lasso1(x,y,intercept=FALSE)
reg.funs[["Relaxed lasso"]] = function(x,y) lasso1(x,y,intercept=FALSE,
relax=T, gamma = 0.5)
reg.funs[["Adaptive Lasso"]] = function(x,y) lasso1(x,y,intercept=FALSE, penalty =T)
reg.funs[["Elastic Net"]] = function(x,y) lasso1(x,y,intercept=FALSE, alpha = 0.05)
# Run the master simulation function, for two different SNRs
sim.obj.hisnr = sim.master(n,p,nval,reg.funs=reg.funs,nrep=20,seed=0, rho = 0.35,
beta.type=2,s=5,snr=3,verbose=TRUE)
sim.obj.losnr = sim.master(n,p,nval,reg.funs=reg.funs,nrep=20,seed=0, rho = 0.35,
beta.type=2,s=5,snr=0.1,verbose=TRUE)
fig1.1 = plot(sim.obj.hisnr, methods = 1:5, main="n = 100, p = 20, beta.type = 2, SNR = 3")
fig1.2 = plot(sim.obj.losnr, methods = 1:5, main="n = 100, p = 20, beta.type = 2, SNR = 0.1")
sim.obj.losnr
fig1.1 = plot(sim.obj.hisnr, method.nums = 1:5, main="n = 100, p = 20, beta.type = 2, SNR = 3")
fig1.2 = plot(sim.obj.losnr, methods.nums = 1:5, main="n = 100, p = 20, beta.type = 2, SNR = 0.1")
fig1.2 = plot(sim.obj.losnr, method.nums = 1:5, main="n = 100, p = 20, beta.type = 2, SNR = 0.1")
grid.arrange(fig1.1, fig1.2, ncol = 2 )
# Regression functions: lasso, forward stepwise, and best subset selection
reg.funs1 = list()
reg.funs1[["Best subset"]] = function(x,y) bs(x,y,intercept=FALSE)
reg.funs1[["Stepwise"]] = function(x,y) fs(x,y,intercept=FALSE)
reg.funs1[["Lasso"]] = function(x,y) lasso1(x,y,intercept=FALSE)
reg.funs1[["Relaxed lasso"]] = function(x,y) lasso0(x,y,intercept=FALSE,
relax=T, gamma = 0.5)
reg.funs[["Adaptive Lasso"]] = function(x,y) lasso1(x,y,intercept=FALSE, penalty =T)
reg.funs[["Elastic Net"]] = function(x,y) lasso1(x,y,intercept=FALSE, alpha = 0.05)
# Regression functions: lasso, forward stepwise, and best subset selection
reg.funs1 = list()
reg.funs1[["Best subset"]] = function(x,y) bs(x,y,intercept=FALSE)
reg.funs1[["Stepwise"]] = function(x,y) fs(x,y,intercept=FALSE)
reg.funs1[["Lasso"]] = function(x,y) lasso1(x,y,intercept=FALSE)
reg.funs1[["Relaxed lasso"]] = function(x,y) lasso0(x,y,intercept=FALSE,
relax=T, gamma = 0.5)
reg.funs[["Adaptive Lasso"]] = function(x,y) lasso1(x,y,intercept=FALSE, penalty =T)
reg.funs[["Elastic Net"]] = function(x,y) lasso1(x,y,intercept=FALSE, alpha = 0.05)
# Run the master simulation function, for two different SNRs
sim.obj.hisnr1 = sim.master(n,p,nval,reg.funs=reg.funs1,nrep=20,seed=0, rho = 0.35,
beta.type=2,s=5,snr=3,verbose=TRUE)
# Regression functions: lasso, forward stepwise, and best subset selection
reg.funs1 = list()
reg.funs1[["Best subset"]] = function(x,y) bs(x,y,intercept=FALSE)
reg.funs1[["Stepwise"]] = function(x,y) fs(x,y,intercept=FALSE)
reg.funs1[["Lasso"]] = function(x,y) lasso1(x,y,intercept=FALSE)
reg.funs1[["Relaxed lasso"]] = function(x,y) lasso(x,y,intercept=FALSE,
relax=T, gamma = 0.5)
reg.funs[["Adaptive Lasso"]] = function(x,y) lasso1(x,y,intercept=FALSE, penalty =T)
reg.funs[["Elastic Net"]] = function(x,y) lasso1(x,y,intercept=FALSE, alpha = 0.05)
# Run the master simulation function, for two different SNRs
sim.obj.hisnr1 = sim.master(n,p,nval,reg.funs=reg.funs1,nrep=20,seed=0, rho = 0.35,
beta.type=2,s=5,snr=3,verbose=TRUE)
# Regression functions: lasso, forward stepwise, and best subset selection
reg.funs1 = list()
reg.funs1[["Best subset"]] = function(x,y) bs(x,y,intercept=FALSE)
reg.funs1[["Stepwise"]] = function(x,y) fs(x,y,intercept=FALSE)
reg.funs1[["Lasso"]] = function(x,y) lasso1(x,y,intercept=FALSE)
reg.funs1[["Relaxed lasso"]] = function(x,y) lasso(x,y,intercept=FALSE,
nrelax = 5)
reg.funs[["Adaptive Lasso"]] = function(x,y) lasso1(x,y,intercept=FALSE, penalty =T)
reg.funs[["Elastic Net"]] = function(x,y) lasso1(x,y,intercept=FALSE, alpha = 0.05)
# Run the master simulation function, for two different SNRs
sim.obj.hisnr1 = sim.master(n,p,nval,reg.funs=reg.funs1,nrep=20,seed=0, rho = 0.35,
beta.type=2,s=5,snr=3,verbose=TRUE)
sim.obj.losnr1 = sim.master(n,p,nval,reg.funs=reg.funs1,nrep=20,seed=0, rho = 0.35,
beta.type=2,s=5,snr=0.1,verbose=TRUE)
fig11.1 = plot(sim.obj.hisnr1, methods = 1:5, main="n = 100, p = 20, beta.type = 2, SNR = 3")
fig11.1 = plot(sim.obj.hisnr1, method.nums = 1:5, main="n = 100, p = 20, beta.type = 2, SNR = 3")
fig11.2 = plot(sim.obj.losnr1, method.nums = 1:5, main="n = 100, p = 20, beta.type = 2, SNR = 0.1")
grid.arrange(fig11.1, fig11.2, ncol = 2 )
# Regression functions: lasso, forward stepwise, and best subset selection
reg.funs1 = list()
reg.funs1[["Best subset"]] = function(x,y) bs(x,y,intercept=FALSE)
reg.funs1[["Stepwise"]] = function(x,y) fs(x,y,intercept=FALSE)
reg.funs1[["Lasso"]] = function(x,y) lasso1(x,y,intercept=FALSE)
reg.funs1[["Relaxed lasso"]] = function(x,y) lasso(x,y,intercept=FALSE,
nrelax = 5)
reg.funs1[["Adaptive Lasso"]] = function(x,y) lasso1(x,y,intercept=FALSE, penalty =T)
reg.funs1[["Elastic Net"]] = function(x,y) lasso1(x,y,intercept=FALSE, alpha = 0.05)
# Run the master simulation function, for two different SNRs
sim.obj.hisnr1 = sim.master(n,p,nval,reg.funs=reg.funs1,nrep=20,seed=0, rho = 0.35,
beta.type=2,s=5,snr=3,verbose=TRUE)
sim.obj.losnr1 = sim.master(n,p,nval,reg.funs=reg.funs1,nrep=20,seed=0, rho = 0.35,
beta.type=2,s=5,snr=0.1,verbose=TRUE)
fig11.1 = plot(sim.obj.hisnr1, method.nums = 1:5, main="n = 100, p = 20, beta.type = 2, SNR = 3")
fig11.2 = plot(sim.obj.losnr1, method.nums = 1:5, main="n = 100, p = 20, beta.type = 2, SNR = 0.1")
grid.arrange(fig11.1, fig11.2, ncol = 2 )
######## FIG 4 ##########
n = 100
p = 10
nval = n
grid.arrange(fig2_sx, fig2_dx, ncol = 2)
sim_fig2
sim_fig2.1
sim_fig2
sim_fig2
# fig 2 right
sim_fig2.1 = sim.master(n,p,nval,reg.funs=reg.funs, rho = 0.7, nrep=20,seed=0,
beta.type=2,s=5, snr = 0.7, verbose=TRUE)
fig2_dx = plot(sim_fig2.1, method.nums = 1:5, what ="risk", main = "SNR = 0.7, Cor=0.7")
grid.arrange(fig2_sx, fig2_dx, ncol = 2)
fig2_sx = plot(sim_fig2, method.nums = 1:5, method.nums = 1:5, what ="risk", main = "SNR = 0.7, Cor=0.2")
sim_fig2
fig2_sx = plot(sim_fig2, method.nums = 1:5,  what ="risk", main = "SNR = 0.7, Cor=0.2")
# fig 2 right
sim_fig2.1 = sim.master(n,p,nval,reg.funs=reg.funs, rho = 0.7, nrep=20,seed=0,
beta.type=2,s=5, snr = 0.7, verbose=TRUE)
fig2_dx = plot(sim_fig2.1, method.nums = 1:5, what ="risk", main = "SNR = 0.7, Cor=0.7")
grid.arrange(fig2_sx, fig2_dx, ncol = 2)
n
p
file_list_fig5
a = plot.from.file(file_list_fig5, main=paste0("n=",n,", p=",p,", s=",s), what = "error" )
setwd("C:/Users/Maria Vittoria/Desktop/EPFL/SCV/main-project-mvk/fig5")
a = plot.from.file(file_list_fig5, main=paste0("n=",n,", p=",p,", s=",s), what = "error" )
b = plot.from.file(file_list_fig5, main=paste0("n=",n,", p=",p,", s=",s), what = "prop")
c = plot.from.file(file_list_fig5, main=paste0("n=",n,", p=",p,", s=",s), what = "nonzero")
d = plot.from.file(file_list_fig5, main=paste0("n=",n,", p=",p,", s=",s), what = "F")
grid.arrange(a, b, ncol = 2) #fig5.1_Norm
grid.arrange(c, d, ncol = 2) #fig5.2_Norm
a = plot.from.file(file_list_fig5, method.nums = 1:5, main=paste0("n=",n,", p=",p,", s=",s), what = "error" )
b = plot.from.file(file_list_fig5, method.nums = 1:5, main=paste0("n=",n,", p=",p,", s=",s), what = "prop")
c = plot.from.file(file_list_fig5, method.nums = 1:5, main=paste0("n=",n,", p=",p,", s=",s), what = "nonzero")
d = plot.from.file(file_list_fig5, method.nums = 1:5, main=paste0("n=",n,", p=",p,", s=",s), what = "F")
grid.arrange(a, b, ncol = 2) #fig5.1_Norm
grid.arrange(c, d, ncol = 2) #fig5.2_Norm
setwd("C:/Users/Maria Vittoria/Desktop/EPFL/SCV/main-project-mvk/fig5_t")
cc = plot.from.file(file_list_fig5_T , main=paste0("n=",n,", p=",p,", s=",s), what = "nonzero")
dd = plot.from.file(file_list_fig5_T , main=paste0("n=",n,", p=",p,", s=",s), what = "F")
grid.arrange(cc, dd, ncol = 2)
setwd("C:/Users/Maria Vittoria/Desktop/EPFL/SCV/main-project-mvk/fig5_t")
for (i in 1:length(snr_val)){
sim.master_t(n,p,nval,reg.funs=reg.funs, df = 2.1, rho = 0.35, nrep=20,seed=0, file = paste0("prova_T", i),
beta.type=2,s=5, snr = snr_val[i],verbose=TRUE)
}
file_list_fig5_T = paste0("prova_T", 1:10)
cc = plot.from.file(file_list_fig5_T , method.nums = 1:5, main=paste0("n=",n,", p=",p,", s=",s), what = "nonzero")
dd = plot.from.file(file_list_fig5_T , method.nums = 1:5, main=paste0("n=",n,", p=",p,", s=",s), what = "F")
grid.arrange(cc, dd, ncol = 2)
save.image("C:/Users/Maria Vittoria/Desktop/EPFL/SCV/main-project-mvk/workspace.RData")
a = plot.from.file(file_list_fig5, method.nums = 1:5, main=paste0("n=",n,", p=",p,", s=",s), legend.pos = "right", what = "error" )
setwd("C:/Users/Maria Vittoria/Desktop/EPFL/SCV/main-project-mvk/fig5")
a = plot.from.file(file_list_fig5, method.nums = 1:5, main=paste0("n=",n,", p=",p,", s=",s), legend.pos = "right", what = "error" )
b = plot.from.file(file_list_fig5, method.nums = 1:5, main=paste0("n=",n,", p=",p,", s=",s), legend.pos = "right", what = "prop")
d = plot.from.file(file_list_fig5, method.nums = 1:5, main=paste0("n=",n,", p=",p,", s=",s),  legend.pos = "right", what = "F")
c = plot.from.file(file_list_fig5, method.nums = 1:5, main=paste0("n=",n,", p=",p,", s=",s),  legend.pos = "right", what = "nonzero")
grid.arrange(a, b, ncol = 2) #fig5.1_Norm
grid.arrange(c, d, ncol = 2) #fig5.2_Norm
grid.arrange(a, b, ncol = 2) #fig5.1_Norm
grid.arrange(c, d, ncol = 2) #fig5.2_Norm
cc = plot.from.file(file_list_fig5_T , method.nums = 1:5, legend.pos = "right", main=paste0("n=",n,", p=",p,", s=",s), what = "nonzero")
setwd("C:/Users/Maria Vittoria/Desktop/EPFL/SCV/main-project-mvk/fig5_t")
cc = plot.from.file(file_list_fig5_T , method.nums = 1:5, legend.pos = "right", main=paste0("n=",n,", p=",p,", s=",s), what = "nonzero")
dd = plot.from.file(file_list_fig5_T , method.nums = 1:5, legend.pos = "right", main=paste0("n=",n,", p=",p,", s=",s), what = "F")
grid.arrange(cc, dd, ncol = 2)
